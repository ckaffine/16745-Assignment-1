# 16745-Assignment-1

## Part 1
I used Matlab's fmincon to do the optimization, providing the joint limits as upper and lower bounds on the parameters and satisfying other constraints throught the cost function. The cost function I used consisted of the distance from the end effector to the target location, the distance between the target orientation and the end effector orientation based on the different between the identity matrix and the product of one rotation with the inverse of the other, and cost functions based on proximity to the obstacles and the joint limits. The cost function for the joint limits is 1 divided by the distance between the actual joint value and the limit, summed over each limit. The cost function for the obstacles is the similar, where the distance used is the shortest distance between the obstacle and the robot. These cost functions work well because they fade to 0 at a reasonable distance away from the obstacle or joint limit, but near the constraint they quickly rise to infinity, so there can never be a situation where a local minimum sits on or behind the boundary. They are also relatively easy to differentiate, which helped with the next part.

## Part 2
For this part, I took the gradient of the cost function by hand to pass into the optimization routines. While calculating the robot's forward kinemeatics, I also calculate the jacobian of each joint location with respect to the joint parameters, as well as the jacobian of the rotation matrix representation of the end effector pose. The jacobians make calculating the gradient fairly straightforward by applying the chain rule appropriately in a few places. Including the derivatives makes the optimization run much faster, although the results appear to be of pretty similar quality.

## Part 3
Of the four methods, active-set seems to provide the fastest and most consistently accurate results. SQP tends to be a bit slower, but similar in quality, while interior-point takes a while to converge and occasionally provides non-ideal results. CMA-ES takes much longer to converge than all of the others, but the results are consistently very good, and in some tricky cases are likely better than any of the others.

## Part 4
To find multiple minima, I ran the optimization algorith multiple times with different random starting points each iteration. Also, to avoid running into the same minimum, I added a term to the cost function penalizing points in parameter space that were too close to any previous solution. The point of this was to "fill in" the minimum as much as possible to force the optimizer to explore other regions of parameter space. How well this works seems to depend a lot on the specific setup. With few obstacles in the way, the result tends to be that the robot gets slightly deflected away from the original solution; basically, the cost function now looks like a valley with a bump in the middle, so the algorithm settles for a point inside the valley at the bottom of the bump. With more obstacles or constraints leading to a more difficult problem, the results can be much better. In these cases, because the problem is more complex the cost function has more local minima, so when the optimizer is pushed away from one potential minimum it is more likely to fall into another, completely different one, and the robot will take a very visibly different path. In general, this behavior seems pretty reasonable, since multiple non-trivial minima are found exactly when the problem is complex and may benefit from having multiple options, whereas in the cases when all solutions are basically the same the problem is already pretty straightforward and the initial solution is probably good enough.
